{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Machine Learning\" Course from [Coursera](https://www.coursera.org/learn/machine-learning/home/week/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Understanding\n",
    "- There are traditional method to solve cost function (squared error function), like normal equations (least square method) in a mathmatical way.\n",
    "- The problem of the traditional method is that computational effort (if thousands, millions parameters), then from a computational way, the gradient descent is more powerful\n",
    "- The idea of gradient descent is to try to find a local minimum, then derivative comes out, firstly get the derivation (like tangent point), secondly find a rate to move the iteration, and then converge into a local minimum (if it's a convex quadratic function, there is only one global minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Linear Regression\n",
    "- **Model Representation**\n",
    "    - Hypothesis with multiple features: <BR>\n",
    "    define $x_{0}^{(i)}$ = 1 for ($i \\in 1,…,m$), <BR>\n",
    "    $x_{j}^{(i)}$ = value of feature $j$ in the $i^{th}$ training example, <BR>\n",
    "    $x^{(i)}$ = the input (features) of the $i^{th}$ training example, <BR>\n",
    "    $m$ = the number of training examples, <BR>\n",
    "    $n$ = the number of features <BR>\n",
    "$h_\\theta{(x)} = \\theta_{0}x_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\theta_{3}x_{3} + \\dots + \\theta_{n}x_{n}$\n",
    "<BR>\n",
    "$x = \\begin{bmatrix}x_0 \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\\end{bmatrix} \\in \\mathbb {R}^{n+1}$,   \n",
    "$\\theta = \\begin{bmatrix}\\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\theta_n\\end{bmatrix} \\in \\mathbb {R}^{n+1}$\n",
    "<BR>\n",
    "Then we can get\n",
    "<BR>\n",
    "$h_\\theta(x) =\\begin{bmatrix}\\theta_0 \\hspace{2em} \\theta_1 \\hspace{2em} ... \\hspace{2em} \\theta_n\\end{bmatrix}\\begin{bmatrix}x_0 \\newline x_1 \\newline \\vdots \\newline x_n\\end{bmatrix}= \\theta^T x$\n",
    "\n",
    "![Multivariate%20Linear%20Regression.png](./W2/Multivariate%20Linear%20Regression.png)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "- Hypothesis: $h_\\theta{(x)} = \\theta^T x = \\theta_{0}x_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\theta_{3}x_{3} + \\dots + \\theta_{n}x_{n}$\n",
    "- Parameters: $\\theta$ equal to $\\theta_0, \\theta_1, \\dots, \\theta_n$\n",
    "- Cost Function or Squared error function: $J(\\theta) = J(\\theta_0, \\theta_1, \\dots, \\theta_n) = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left ( \\hat{y}^{(i)}- y^{(i)} \\right)^2 = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left (h_\\theta (x^{(i)}) - y^{(i)} \\right)^2$\n",
    "- Goal: minimize $J(\\theta)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "- Apply Gradient descent algorithm to the cost function $ \\dfrac{∂}{∂\\theta_{j}} J(\\theta) =  \\dfrac{∂}{∂\\theta_{j}} \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left (h_\\theta (x^{(i)}) - y^{(i)} \\right)^2$\n",
    "- The gradient descent equation itself is generally the same form; we just have to repeat it for our 'n' features:\n",
    "<br>\n",
    "repeat until convergence {\n",
    "<br>\n",
    "$\\theta_{j} := \\theta_{j} - \\alpha \\dfrac {1}{m} \\displaystyle \\sum _{i=1}^m \\left (h_\\theta (x^{(i)}) - y^{(i)} \\right) \\cdot x_{j}^{(i)}$\n",
    "<br>\n",
    "$\\text{for j := 0...n} \\rbrace$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent in Practice I - Feature Scaling\n",
    "- As the scale of the features are very different, it might take a long time to run gradient dscent, to speed up we can have the features in roughly the same range. _(This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.)_ \n",
    "- Two techniques to help with this are **feature scaling** and **mean normalization**\n",
    "<BR> $x_i := \\dfrac {x_i - \\mu_i}{s_i}$\n",
    "<BR>Where $\\mu_i$ is the average of all the values for feature ($i$) and $s_i$ is the range of values (max - min), or $s_i$ is the standard deviation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent in Practice II - Learning Rate\n",
    "- **Debugging gradient descent**: Make a plot with number of iterations on the x-axis. Now plot the cost function, $J(\\theta)$ over the number of iterations of gradient descent. If $J(\\theta)$ ever increases, then you probably need to decrease α\n",
    "- To summarize: \n",
    "    - For sufficiently small $\\alpha$, $J(\\theta)$ should decrease on every iteration\n",
    "    - If $\\alpha$ is too small: slow convergence.\n",
    "    - If $\\alpha$ is too large: may not decrease on every iteration and thus may not converge.\n",
    "    - Try to plot with number of iterations, and choose by below threshold, ideally choose one just below the max threshold\n",
    "\n",
    "\n",
    "![Learning%20Rate.png](./W2/Learning%20Rate.png)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and Polynomial Regression\n",
    "- Sometimes we can determine the features, for example, instead of choose frontage and depth as two features to predict house price, we can use area (frontage * depth) as one feature to better fit the model\n",
    "- For some curves, we can use Polynomial Regression (quadratic function, cubic function, sqare root function, etc)\n",
    "    - Sqare root function: $h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 \\sqrt{x_1} $\n",
    "    - Cubic function: $h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_1^2 + \\theta_3 x_1^3 $\n",
    "- If do so, don't forget to do feature scaling\n",
    "\n",
    "![Polynomial%20Regression.png](./W2/Polynomial%20Regression.png)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Equation\n",
    "- Normal Equation (least square method) can allow us to find the optimum theta without iteration.\n",
    "    - The normal equation formula: $\\theta = (X^T X)^{-1}X^Ty$\n",
    "    - $X$ is a $m*n$ matrice, $m$ is number of observersation, $n$ is number of features\n",
    "<BR>\n",
    "\n",
    "|Gradient Descent\t|Normal Equation|\n",
    "|-----------|-----------|\n",
    "|Need to choose alpha\t|No need to choose alpha|\n",
    "|Needs many iterations  |No need to iterate|\n",
    "|Need to do feature scaling\t|No need to do feature scaling|\n",
    "|Computations $O(kn^2)$\t|Computations $O(n^3)$, need to calculate inverse of $X^T X$|\n",
    "|Works well when n is large\t|Slow if n is very large|\n",
    "<BR>\n",
    "\n",
    "- In practice, when n exceeds 10,000 it might be a good time to go from a normal solution to an iterative process\n",
    "- In some cases, $X^T X$ might be **noninvertible**: 1) **Redundant features**, where two features are very closely related (i.e. they are linearly dependent); 2) **Too many features** (e.g. m ≤ n). In this case, delete some features or use \"regularization\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with multiple variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32.07273388]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# x = np.loadtxt('./W2/ex1data2.txt',delimiter=',')\n",
    "df = pd.read_csv('./W2/ex1data1.txt',delimiter=',', header=None)\n",
    "\n",
    "\n",
    "\n",
    "x = df.iloc[:,:1].to_numpy()\n",
    "y = df.iloc[:,1:].to_numpy()\n",
    "\n",
    "def normalization(arr):\n",
    "    mn = np.mean(arr, axis=0)\n",
    "    sd = np.std(arr, axis=0)\n",
    "    m = arr.shape[0]\n",
    "    \n",
    "    norm = (arr - np.ones((m, 1)).dot([mn])) / np.ones((m, 1)).dot([sd])\n",
    "    \n",
    "\n",
    "    \n",
    "    return mn, sd, norm\n",
    "\n",
    "\n",
    "\n",
    "def costfunction(x, y, theta):\n",
    "    \n",
    "    m = y.shape[0] # number of training examples\n",
    "\n",
    "    J = 0\n",
    "\n",
    "    se = x @ theta - y\n",
    "\n",
    "    J = se.T @ se / (2*m)\n",
    "    \n",
    "    return J\n",
    "\n",
    "mn, sd, x_norm = normalization(x)\n",
    "\n",
    "theta = np.array([[0]])\n",
    "\n",
    "costfunction(x_norm, y, theta)\n",
    "\n",
    "\n",
    "# x, y\n",
    "\n",
    "# # Create a matrice\n",
    "# A = np.array([[1, 2, 3],[4, 5, 6],[7,8,9],[10,11,12]])\n",
    "# print(A)\n",
    "\n",
    "# # Initialize a vector \n",
    "# v = np.array([[1], [2], [3]])\n",
    "# print(v)\n",
    "\n",
    "# # Get the dimension of the matrix A where m = rows and n = columns\n",
    "# dim_A = A.shape\n",
    "# print(dim_A)\n",
    "\n",
    "# # Get the dimension of the vector v \n",
    "# dim_v = v.shape\n",
    "# print(dim_v)\n",
    "\n",
    "# # Now let's index into the 2nd row 3rd column of matrix A\n",
    "# A_23 = A[1][2]\n",
    "# a_23 = A[1,2]\n",
    "# print(A_23, a_23)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
