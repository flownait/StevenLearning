{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Machine Learning\" Course from [Coursera](https://www.coursera.org/learn/machine-learning/home/welcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Understanding\n",
    "- There are traditional method to solve cost function (squared error function), like normal equations (least square method) in a mathmatical way.\n",
    "- The problem of the traditional method is that computational effort (if thousands, millions parameters), then from a computational way, the gradient descent is more powerful\n",
    "- The idea of gradient descent is to try to find a local minimum, then derivative comes out, firstly get the derivation (like tangent point), secondly find a rate to move the iteration, and then converge into a local minimum (if it's a convex quadratic function, there is only one global minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- **Supervised Learning**\n",
    "    * The term **Supervised Learning** refers to the fact that we gave the algorithm a data set in which the, called, \"right answers\" were given.\n",
    "        * **Regression** problem (predict house price) or **classification** problem (determine a malignant or benign tumor).\n",
    "        * In supervised learning, we are **given a data set** and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.\n",
    "        * Supervised learning problems are categorized into \"regression\" and \"classification\" problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories. \n",
    "- **Unsupervised Learning**\n",
    "    * In **Unsupervised Learning**, we're given data that looks different than data that looks like this that **doesn't have any labels** or that all has the same label or really no labels. We don't give the algrithim / the right answer in advance.\n",
    "        * **Cluster** problem (determine market segmentation) or **cocktail party** problem (segragate voice recording from 2 audio sources)\n",
    "        * SVD fnction stands for singular value decomposition\n",
    "![Introduction%20-%20Unsupervised%20Learning%20-%20Cocktail%20party%20problem%20algorithm.png](./W1/Introduction%20-%20Unsupervised%20Learning%20-%20Cocktail%20party%20problem%20algorithm.png)\n",
    "        * Unsupervised learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don't necessarily know the effect of the variables. We can derive this structure by clustering the data based on relationships among the variables in the data. With unsupervised learning there is no feedback based on the prediction results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Cost Function\n",
    "- **Model Representation**\n",
    "    * Notation - m: traning examples, x's: input, y's: output, A pair $(x^{(i)} , y^{(i)})$ is called a training example\n",
    "    ![Model%20and%20Cost%20Function%20-%20Notation.png](./W1/Model%20and%20Cost%20Function%20-%20Notation.png)\n",
    "    * univariate linear regression: One variable regression model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "- Hypothesis: $h_\\theta{(x)} = \\theta_{0} + \\theta_{1}*x$\n",
    "- Parameters: $\\theta_0, \\theta_1$\n",
    "- Cost Function or Squared error function: $J(\\theta_0, \\theta_1) = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left ( \\hat{y}_{i}- y_{i} \\right)^2 = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left (h_\\theta (x_{i}) - y_{i} \\right)^2$\n",
    "- Goal: minimize $J(\\theta_{0},\\theta_{1})$\n",
    "- Square cost function is probably the most commonly used one for regression problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Cost Function\n",
    "- $h_\\theta{x} = \\theta_{1}x$, assumed $\\theta_{0}$=0\n",
    "- Then $J(\\theta_{1}) = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left (h_\\theta (x_{i}) - y_{i} \\right)^2 = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left (\\theta_{1} x_{i} - y_{i} \\right)^2$\n",
    "\n",
    "- Then Cost Function is function of parameter $\\theta_{1}$\n",
    "    - If $\\theta_{1}$ = 1, then $J(\\theta_{1}) = J(1) = \\frac{1}{2m}\\displaystyle\\sum_{i=1}^{m}(x_{i}-y_{i})^{2}$\n",
    "    - If $\\theta_{1}$ = 0, then $J(\\theta_{1}) = J(0) = \\frac{1}{2m}\\displaystyle\\sum_{i=1}^{m}(y_{i})^{2}$\n",
    "- So to minimize J Cost Function of $\\theta_{1}$ is likely to find the minimum value of parabola function of $\\theta_{1}$\n",
    "    - We can infer as below $\\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left (\\theta_{1} x_{i} - y_{i} \\right)^2 = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left (\\theta_{1}^2 x_{i}^2 - 2 \\theta_{1} x_{i} y_{i} + y_{i}^2\\right) = \\dfrac {1}{2m} \\displaystyle (\\theta_{1}^2 \\sum _{i=1}^m x_{i}^2 - 2 \\theta_{1} \\sum _{i=1}^m  x_{i} y_{i} + \\sum _{i=1}^m y_{i}^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Variable Cost Function\n",
    "- $h_\\theta{x} = \\theta_{0} + \\theta_{1}x$, if $\\theta_{0}$<>0\n",
    "- Then $J(\\theta_{1}, \\theta_{1}) = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left (h_\\theta (x_{i}) - y_{i} \\right)^2 = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left (\\theta_{0} + \\theta_{1} x_{i} - y_{i} \\right)^2$\n",
    "\n",
    "- Then Cost Function is function of parameter $\\theta_{0}, \\theta_{1}$\n",
    "    - If $\\theta_{0} = 1, \\theta_{1}$ = 1, then $J(\\theta_{0}, \\theta_{1}) = J(1, 1) = \\frac{1}{2m}\\displaystyle\\sum_{i=1}^{m}(1+x_{i}-y_{i})^{2}$\n",
    "    - If $\\theta_{0} = 1, \\theta_{1}$ = 0, then $J(\\theta_{0}, \\theta_{1}) = J(1, 0) = \\frac{1}{2m}\\displaystyle\\sum_{i=1}^{m}(1+y_{i})^{2}$\n",
    "- So to minimize J Cost Function of $\\theta_{1}$ is likely to find the minimum value of parabola function of $\\theta_{1}$\n",
    "    - We can infer as below $\\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left (\\theta_{0} + \\theta_{1} x_{i} - y_{i} \\right)^2 = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left (\\theta_{0}^2 + \\theta_{1}^2 x_{i}^2 + y_{i}^2 + 2 \\theta_{0} \\theta_{1} x_{i} - 2 \\theta_{0} y_{i} - 2 \\theta_{1} x_{i} y_{i} \\right)$\n",
    "    \n",
    "    ![Cost%20Function%20Intuition%20II.png](./W1/Cost%20Function%20Intuition%20II.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Learning\n",
    "### Gradient descent \n",
    "\n",
    "### Problem Setup\n",
    "- For function $J(\\theta_{0}, \\theta_{1})$, want $\\displaystyle \\min_{\\theta_{0}, \\theta_{1}} J(\\theta_{0}, \\theta_{1})$\n",
    "\n",
    "### Outline\n",
    "- Start with some $\\theta_{0}, \\theta_{1}$ (Initialize for example $\\theta_{0}=0, \\theta_{1}=0$)\n",
    "- Keep changing $\\theta_{0}, \\theta_{1}$ to reduce $J(\\theta_{0}, \\theta_{1})$ until we hopefully end up at a minimum\n",
    "\n",
    "### Algorithm\n",
    "- repeat until convergence {\n",
    "$\\theta_{j} := \\theta_{j} - \\alpha \\frac{∂}{∂\\theta_{j}} J(\\theta_{0}, \\theta_{1}) $ \n",
    "}\n",
    "    - := is assignment operator\n",
    "    - $\\alpha$ is a number called the learning rate, it basically controls how big a step we take downhill with creating descent, always a postive number; If $\\alpha$ is too small, gradient descent can be slow, if $\\alpha$ is too large, gradient descent can overshoot the minimum and may fail to converge, or even diverge\n",
    "    - Need to update $\\theta_{0}$, $\\theta_{1}$ simultaneously\n",
    "    - $\\alpha$ can be fixed, and gradient descent can still converge to a local minimum, this is because the derivative term is the tangent point, it will become smaller and smaller when close to 0.\n",
    "\n",
    "    ![gradient%20descent.png](./W1/gradient%20descent.png)\n",
    "    ![gradient%20descent%20intuition.png](./W1/gradient%20descent%20intuition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent For Linear Regression\n",
    "- Apply Gradient descent algorithm to squared error cost function, \n",
    "- and then get cost function $ \\dfrac{∂}{∂\\theta_{j}} J(\\theta_{0}, \\theta_{1}) =  \\dfrac{∂}{∂\\theta_{j}} \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left (h_\\theta (x_{i}) - y_{i} \\right)^2$\n",
    "- and then substitute actual cost function and actual hypothesis function and modify the equation to:\n",
    "<br>\n",
    "repeat until convergence {\n",
    "<br>\n",
    "$\\theta_{0} := \\theta_{0} - \\alpha \\dfrac {1}{m} \\displaystyle \\sum _{i=1}^m \\left (h_\\theta (x_{i}) - y_{i} \\right)$ \n",
    "<br>\n",
    "$\\theta_{1} := \\theta_{1} - \\alpha \\dfrac {1}{m} \\displaystyle \\sum _{i=1}^m \\left ((h_\\theta (x_{i}) - y_{i} \\right)x_{i})$\n",
    "<br>\n",
    "}\n",
    "- \"Batch\" Gradient Descent: Each step of gradient descent uses all the training examples.\n",
    "\n",
    "    ![Gradient%20Descent%20For%20Linear%20Regression.png](./W1/Gradient%20Descent%20For%20Linear%20Regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n",
      "[[1]\n",
      " [2]\n",
      " [3]]\n",
      "(4, 3)\n",
      "(3, 1)\n",
      "6 6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a matrice\n",
    "A = np.array([[1, 2, 3],[4, 5, 6],[7,8,9],[10,11,12]])\n",
    "print(A)\n",
    "\n",
    "# Initialize a vector \n",
    "v = np.array([[1], [2], [3]])\n",
    "print(v)\n",
    "\n",
    "# Get the dimension of the matrix A where m = rows and n = columns\n",
    "dim_A = A.shape\n",
    "print(dim_A)\n",
    "\n",
    "# Get the dimension of the vector v \n",
    "dim_v = v.shape\n",
    "print(dim_v)\n",
    "\n",
    "# Now let's index into the 2nd row 3rd column of matrix A\n",
    "A_23 = A[1][2]\n",
    "a_23 = A[1,2]\n",
    "print(A_23, a_23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 4]\n",
      " [5 3 2]] \n",
      " [[1 3 4]\n",
      " [1 1 1]]\n",
      "[[2 5 8]\n",
      " [6 4 3]]\n",
      "[[ 0 -1  0]\n",
      " [ 4  2  1]]\n",
      "[[ 2  4  8]\n",
      " [10  6  4]]\n",
      "[[0.5 1.  2. ]\n",
      " [2.5 1.5 1. ]]\n",
      "[[3 4 6]\n",
      " [7 5 4]]\n"
     ]
    }
   ],
   "source": [
    "######### Addition and Scalar Multiplication #########\n",
    "# Initialize matrix A and B \n",
    "A = np.array([[1, 2, 4], [5, 3, 2]])\n",
    "B = np.array([[1, 3, 4], [1, 1, 1]])\n",
    "print(A,'\\n', B)\n",
    "\n",
    "# Initialize constant s \n",
    "s = 2\n",
    "\n",
    "# See how element-wise addition works\n",
    "add_AB = A + B \n",
    "print(add_AB)\n",
    "\n",
    "# See how element-wise subtraction works\n",
    "sub_AB = A - B\n",
    "print(sub_AB)\n",
    "\n",
    "# See how scalar multiplication works\n",
    "mult_As = A * s\n",
    "print(mult_As)\n",
    "\n",
    "# Divide A by s\n",
    "div_As = A / s\n",
    "print(div_As)\n",
    "\n",
    "# What happens if we have a Matrix + scalar?\n",
    "add_As = A + s\n",
    "print(add_As)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "[[ 6]\n",
      " [15]\n",
      " [24]]\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[[1]\n",
      " [2]]\n",
      "[[ 5]\n",
      " [11]\n",
      " [17]]\n"
     ]
    }
   ],
   "source": [
    "######### Matrix-Vector or Matrx-Matrix Multiplication #########\n",
    "\n",
    "# Initialize matrix A \n",
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(A)\n",
    "\n",
    "# Initialize vector v \n",
    "v = np.array([[1], [1], [1]])\n",
    "print(v)\n",
    "\n",
    "# Multiply A * v\n",
    "Av = A.dot(v)\n",
    "print(Av)\n",
    "\n",
    "# Initialize a 3 by 2 matrix \n",
    "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "print(A)\n",
    "\n",
    "# Initialize a 2 by 1 matrix \n",
    "B = np.array([[1], [2]])\n",
    "print(B)\n",
    "\n",
    "# We expect a resulting matrix of (3 by 2)*(2 by 1) = (3 by 1) \n",
    "mult_AB = A.dot(B)\n",
    "print(mult_AB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [4 5]] \n",
      " [[1 1]\n",
      " [0 2]]\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "[[1. 2.]\n",
      " [4. 5.]]\n",
      "[[1. 2.]\n",
      " [4. 5.]]\n",
      "[[ 1  5]\n",
      " [ 4 14]]\n",
      "[[ 5  7]\n",
      " [ 8 10]]\n"
     ]
    }
   ],
   "source": [
    "######### Matrix Multiplication Properties #########\n",
    "\n",
    "# Initialize random matrices A and B \n",
    "A = np.array([[1,2],[4,5]])\n",
    "B = np.array([[1,1],[0,2]])\n",
    "print(A,'\\n',B)\n",
    "\n",
    "# Initialize a 2 by 2 identity matrix\n",
    "I = np.eye(2)\n",
    "print(I)\n",
    "\n",
    "# The above notation is the same as I = [1,0;0,1]\n",
    "\n",
    "# What happens when we multiply I*A ? \n",
    "IA = I.dot(A)\n",
    "print(IA)\n",
    "\n",
    "# How about A*I ? \n",
    "AI = A.dot(I)\n",
    "print(AI)\n",
    "\n",
    "# Compute A*B \n",
    "AB = A.dot(B)\n",
    "print(AB)\n",
    "\n",
    "# Is it equal to B*A? \n",
    "BA = B.dot(A)\n",
    "print(BA)\n",
    "\n",
    "# Note that IA = AI but AB != BA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 0]\n",
      " [0 5 6]\n",
      " [7 0 9]]\n",
      "[[1 0 7]\n",
      " [2 5 0]\n",
      " [0 6 9]]\n",
      "[[ 0.34883721 -0.13953488  0.09302326]\n",
      " [ 0.3255814   0.06976744 -0.04651163]\n",
      " [-0.27131783  0.10852713  0.03875969]]\n",
      "[[ 1. -0.  0.]\n",
      " [-0.  1. -0.]\n",
      " [-0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "######### Matrix Inverse and Transpose #########\n",
    "\n",
    "# Initialize matrix A \n",
    "A = np.array([[1,2,0], [0,5,6], [7,0,9]])\n",
    "print(A)\n",
    "\n",
    "# Transpose A \n",
    "A_trans = A.T\n",
    "print(A_trans)\n",
    "\n",
    "# Take the inverse of A \n",
    "A_inv = np.linalg.inv(A)\n",
    "print(A_inv)\n",
    "\n",
    "# What is A^(-1)*A? \n",
    "A_invA = A_inv@A # @ is equivalent to np.dot as of Python 3.5\n",
    "print(A_invA.astype(np.float16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are other practices, not necessarily related to the course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Other practices\n",
    "\n",
    "x = np.array([3,2,4,0])\n",
    "y = np.array([4,1,3,1])\n",
    "# y_e = 0 + 1*x\n",
    "# y_e\n",
    "sum((0+1*x-y)**2)/8\n",
    "\n",
    "u = np.array([1,3,-1])\n",
    "v = np.array([2,2,4])\n",
    "u.T@v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(-2,1,500)\n",
    "\n",
    "y = x**2 + x + 10\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
