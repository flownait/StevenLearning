{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Machine Learning Duke University\" Course from [Coursera](https://www.coursera.org/learn/machine-learning-duke/home/week/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "- **References**\n",
    "    * MNIST Data Set of Handwritten Digits (Images): http://yann.lecun.com/exdb/mnist/\n",
    "    * Math for Data Science Course: https://www.coursera.org/learn/datasciencemathskills\n",
    "    * Python Prerequisites: https://www.coursera.org/learn/machine-learning-duke/ungradedLab/YRIb5/python-prerequisites/lab\n",
    "    * PyTorch Installation: https://www.coursera.org/learn/machine-learning-duke/ungradedLab/F1H0x/pytorch-installation/lab\n",
    "    * Coding Environments: https://www.coursera.org/learn/machine-learning-duke/ungradedLab/CmUNS/coding-environments/lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts\n",
    "- **What and Why Machine Learning**: \n",
    "    * Significant improvement since 2012\n",
    "    * in medicine, ophthalmology or dermatology which are heavily based upon image analysis. Machine learning has in some cases **exceeded** the performance of medical doctors. Another area where machine learning in recent years has generated remarkable performance is in playing sophisticated games. While the games themselves are perhaps not of significant interest, the ability of a machine to solve a complex, sequential problem, which is what is represented by a game. And to perform that task in competition with a human and to beat a human is remarkable. The game Go is an ancient game, primarily played in Asia, it was long believed that a machine could not, could not beat the best human at this game. In the last few years, deep learning has shown a performance on the game Go that exceeds the performance of the best players of Go, best human players of Go in the world. \n",
    "    * The key thing to take away from this is that we have training examples which correspond to data and outcomes that we would like to model to predict. Based upon that training data we then apply that training data to a model, that model is typically characterized by a set of parameters.\n",
    "    * __Learning is actually to learn the model parameters__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Logistic Regression**\n",
    "    * Sigmoid function, always lives between zero and one\n",
    "    * The sigmoid function is a way for us to convert predictions about the outcome on a given day into a probabilistic perspective.\n",
    "    ![Logistic%20Regression%20Formula.png](./Week%201/Logistic%20Regression%20Formula.png)\n",
    "    * The parameters B1, B2, B3, tell us how important those data variables are to the prediction and then finally that Z is sent into the Sigmoid function\n",
    "    ![Logistic%20Regression%20Concept.png](./Week%201/Logistic%20Regression%20Concept.png)\n",
    "    * Handwriting Problem, we can say with a certain probability that we are looking at a 1 digit 1, or a digit 0, so what we mean by learning is to infer the set of parameters b0 through bm\n",
    "    ![Logistic%20Regression%20Handwriting.png](./Week%201/Logistic%20Regression%20Handwriting.png)\n",
    "    * Inner product for the interpretation\n",
    "    ![Logistic%20Regression%20Interpretation.png](./Week%201/Logistic%20Regression%20Interpretation.png)\n",
    "    * The interpretation of handwriting learning\n",
    "    ![Logistic%20Regression%20Interpretation%202.png](./Week%201/Logistic%20Regression%20Interpretation%202.png)\n",
    "    * This concept of data in a product with a filter and then sent through a logistic function or the sigmoid function is at the heart of logistic regression. What we'll do as we move forward is to use this concept from logistic regression to move to more sophisticated models that we'll find in deep learning.\n",
    "    * Limitations of Logistic Regression: The key thing to take away is that the logistic regression, which is a very nice model, it has a limitation that it only works effectively whenever a **linear classifier** is effective at distinguishing class one from class zero. Any other case such as the one you're looking at here in which the **decision boundary is more complicated** for which a linear classifier will not be effective.\n",
    "    ![Logistic%20Regression%20Limitations.png](./Week%201/Logistic%20Regression%20Limitations.png)![Logistic%20Regression%20Limitations%20%28Non-Linear%20Classifier%29.png](./Week%201/Logistic%20Regression%20Limitations%20%28Non-Linear%20Classifier%29.png)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Multilayer Perceptron**\n",
    "    * It is actually a rather direct or natural extension from logistic regression. To be viewed as logistic regression on *K* latent features, rather than directly on the *M* components of raw data\n",
    "    ![W1.2.1.Multilayer%20Perceptron%20-%20Extended%20Logistic%20Regression.png](./Week%201/W1.2.1.Multilayer%20Perceptron%20-%20Extended%20Logistic%20Regression.png)\n",
    "    * If we go back to our multilayer perceptron, we can take our data, which here corresponds to the pixels of the image, and instead of multiplying them by filters that correspond to one average four, we might consider k equal to three filters, which correspond to four different ways in which one might draw the number four\n",
    "    ![W1.2.2.Multilayer%20Perceptron%20-%20Extended%20Logistic%20Regression%20Example.png](./Week%201/W1.2.2.Multilayer%20Perceptron%20-%20Extended%20Logistic%20Regression%20Example.png)\n",
    "    * We have a mapping for each of those k-features to a probability from 0-1. So, that basically represents the degree, or the degree to which each of those latent features a is represented in the data. Then, those k-latent features or, those k-latent probabilities are then sent through a logistic regression type model in the same way that we did previously with logistic regression, and now we have a single template filter C with which we take the inner product of those latent features, we get a variable Zeta sub i (the Greek symbol is called Zeta). The key thing to notice about what we're doing with the multilayer perceptron is that instead of taking the data xi, which has m components xi1 through xim, and then directly using logistic regression to map that to a probability of a binary outcome. We have an intermediate step where we introduce k-latent features, and then those k-latent features are then subsequently sent through a logistic regression type model to ultimately give us a probability of a binary outcome\n",
    "    ![W1.2.3.Multilayer%20Perceptron%20-%20Extended%20Logistic%20Regression%20Math%20Model%20Layer%201.png](./Week%201/W1.2.3.Multilayer%20Perceptron%20-%20Extended%20Logistic%20Regression%20Math%20Model%20Layer%201.png)\n",
    "    ![W1.2.4.Multilayer%20Perceptron%20-%20Extended%20Logistic%20Regression%20Math%20Model%20Layer%202.png](./Week%201/W1.2.4.Multilayer%20Perceptron%20-%20Extended%20Logistic%20Regression%20Math%20Model%20Layer%202.png)\n",
    "    * Deep learning is a form of machine learning where a model has multiple layers of latent processes, like below 2 layers of latent processes or filters.\n",
    "    ![W1.2.5.Multilayer%20Perceptron%20-%20Extended%20to%20Deep%20Learning.png](./Week%201/W1.2.5.Multilayer%20Perceptron%20-%20Extended%20to%20Deep%20Learning.png)\n",
    "    * Generally, for a simple logistic regression, one of those limitations was that the decision boundary between the two the types of data, the Y equal one and Y equals zero was required to be linear, a line. So, with this more complicated multilayer perceptron, it turns out that we can learn decision boundaries which are far more sophisticated. This is the key why we would go to such a complicated model.\n",
    "    * One of the applications is to do the document analysis, x for word frequency, y for liked (1) or disliked (0)\n",
    "    ![W1.2.6.Multilayer%20Perceptron%20-%20Document%20Analysis%20Application.png](./Week%201/W1.2.6.Multilayer%20Perceptron%20-%20Document%20Analysis%20Application.png)\n",
    "    ![W1.2.7.Multilayer%20Perceptron%20-%20Document%20Analysis%20Model.png](./Week%201/W1.2.7.Multilayer%20Perceptron%20-%20Document%20Analysis%20Model.png)\n",
    "    * Furthermore, to interpretate the model, below is the example\n",
    "    ![W1.2.8.Multilayer%20Perceptron%20-%20Document%20Analysis%20Model%20Intepretation.png](./Week%201/W1.2.8.Multilayer%20Perceptron%20-%20Document%20Analysis%20Model%20Intepretation.png)\n",
    "    * Transfer Learning is to use all people data for paramater, and then to reuse the parameter to predict other people\n",
    "    ![W1.2.9.Multilayer%20Perceptron%20-%20Document%20Analysis%20Model%20Intepretation%202.png](./Week%201/W1.2.9.Multilayer%20Perceptron%20-%20Document%20Analysis%20Model%20Intepretation%202.png)\n",
    "    ![W1.2.10.Multilayer%20Perceptron%20-%20Document%20Analysis%20Transfer%20Learning.png](./Week%201/W1.2.10.Multilayer%20Perceptron%20-%20Document%20Analysis%20Transfer%20Learning.png)\n",
    "    * Model selection between simple logistic regression and multilayer perceptron is to determine Bias-Variance Trade-Off and performance determination. Logistic regression has less varaince between with different training of N samples of data, but undermine performance (less accurancy)\n",
    "    ![W1.2.11.Multilayer%20Perceptron%20-%20Model%20Selection%20Simple%20Logistic%20Regression.png](./Week%201/W1.2.11.Multilayer%20Perceptron%20-%20Model%20Selection%20Simple%20Logistic%20Regression.png)\n",
    "    ![W1.2.12.Multilayer%20Perceptron%20-%20Model%20Selection%20Multilayer%20Perceptron.png](./Week%201/W1.2.12.Multilayer%20Perceptron%20-%20Model%20Selection%20Multilayer%20Perceptron.png)\n",
    "    * Neural Network was renamed as Deep learning in 2010. It advanced with CNN (1989) + GPU + ImageNet. Indeed, the underlying deep learning technology really was the convolutional neural network.\n",
    "    ![W1.2.13.Multilayer%20Perceptron%20-%20History.png](./Week%201/W1.2.13.Multilayer%20Perceptron%20-%20History.png)\n",
    "    * There's a very famous idea called Ockham's razor (\"All things being equal, the simplest solution tends to be the best one.\") and what it basically says is that if you have two ways of describing data in other words, if you have two models of data and they are equally good at describing the data, you should always choose the simpler model. One should always look for simplicity, and so when we look at the simple but often effective logistic regression and the more complicated Deep Neural Network, one must always recognize that it's probably a good idea to look at data from both perspectives and where the simple model works, the simple model should be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Convolutional Neural Networks**\n",
    "    * The key thing is that we believe that data can be represented in a hierarchy. What we gonna do is to try to build a model that captures this structure, that captures this representation of images. For example as below, each of the motifs at layer three is composed of a subset of shifted versions of the sub-motifs and then each of the sub-motifs is composed of basic, what I'll call atomic elements which are these fundamental shapes. Then the question is, how can we build a model that we can capture the intuition in the way that images might be constituted through a hierarchy of this form.\n",
    "    ![W1.3.1.Convolutional%20Nerual%20Networks%20-%20Hierarchical%20Structure%20of%20Images.png](./Week%201/W1.3.1.Convolutional%20Nerual%20Networks%20-%20Hierarchical%20Structure%20of%20Images.png)\n",
    "    * A **correlation which measures how similar the local region in the images to our atomic element**, which we'll call a **filter**. when we shift that atomic element to every possible position in two-dimensional space of the image, when the atomic element lines up with a point in the image where that triangle is actually manifested, we get a high correlation or a high match. What we're looking at on the top is a map, we call it a **feature map**, and what it is meant to depict is **the degree of match or the degree of correlation between the atomic filter** at bottom right, with every location in the image. __The process by which you shift that atomic element the triangle__, to every location in the image that process has a name is called **convolution**. So, this is why the name convolutional neural network. The convolution is manifested by shifting that filter to every location in the image. That layer three feature map is our final feature map and then based upon those features, we're going to make a **classification** decision. If we repeat these process to layer 1 filters, layer 2 filters, layer 3 filters, we will get a deep architecture. So this **_architecture is called a convolutional neural network_**. The **_convolution is manifested through the shifting process_**.\n",
    "    ![W1.3.2.Convolutional%20Nerual%20Networks%20-%20Lay%201%20Feature%20Maps.png](./Week%201/W1.3.2.Convolutional%20Nerual%20Networks%20-%20Lay%201%20Feature%20Maps.png)\n",
    "    ![W1.3.3.Convolutional%20Nerual%20Networks%20-%20Lay%202%20Feature%20Maps.png](./Week%201/W1.3.3.Convolutional%20Nerual%20Networks%20-%20Lay%202%20Feature%20Maps.png)\n",
    "    ![W1.3.5.Convolutional%20Nerual%20Networks%20-%20CNN.png](./Week%201/W1.3.5.Convolutional%20Nerual%20Networks%20-%20CNN.png)\n",
    "    * I sub n is the nth image. Instead of atomic elements, we use **parameters** called phi 1, phi 2, through phi k correspond to k filters. The parameters of these filters are **just the pixel values**. So, what we're going to try to do is learn the pixel values associated with filter phi 1, filter phi 2, and filter phi k. The feature map is called M sub n. **Layer 2 paramters** psi 1, psi 2, through psi k. They are **digital mini stack of images**. The layer 2 feature map is called L sub n. **Layer 3 parameters*** omega 1 through omega k. Then we get layer 3 feature map at the top layer called G sub n. Finally based upon the features at the top, we can build a **classifier** called script L which will provide a label for that image. The classifier could be a multilayer perceptron or it could be a logistic regression. But in any case, it's characterized by some parameters which we denote by w. Assusmed we have labeled data with actual value (-1,1), then compare the actual value versus predict value from our model. The goal is trying to minimize the variance between actual and prediction, called risk/loss function.\n",
    "    ![W1.3.7.Convolutional%20Nerual%20Networks%20-%20CNN%20Math%20Model%202.png](./Week%201/W1.3.7.Convolutional%20Nerual%20Networks%20-%20CNN%20Math%20Model%202.png)\n",
    "    * The advantage of this deep hierarchical architecture is to share structure. The learning is manifested by taking large quantities of labelled data, learning is the concept of trying to estimate the model parameters, such that the predictions of our model are consistent with the labels, the true labels of the data. \n",
    "    ![W1.3.8.Convolutional%20Nerual%20Networks%20-%20Advantages%20of%20Hierarchical%20Features.png](./Week%201/W1.3.8.Convolutional%20Nerual%20Networks%20-%20Advantages%20of%20Hierarchical%20Features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Applications in the Real World**\n",
    "    * For the real images, the filters, which are learned entirely based upon the data, is that at the **first layer**, the most fundamental layer, the layer one filters, correspond to **building block filters** which are consistent with what seems to be the fundamental means of exciting the neuro architecture of the visual system of mammals. At the **layer 2**, which is at the bottom left, we see **more sophisticated structure**. If you look closely, you'll see things like eyeballs. Finally, at the higher level **layer 3** where we now see more sophisticated structure, we can actually see sketches of faces.\n",
    "    ![W1.4.1.Applications%20in%20the%20Real%20World%20-%20CNN%20on%20Real%20Images.png](./Week%201/W1.4.1.Applications%20in%20the%20Real%20World%20-%20CNN%20on%20Real%20Images.png)\n",
    "    * DeepMind developed the technology to play Go game, which essentially is based upon a deep convolutional neural network, and some additional technology which is put on the top of that, which is called reinforcement learning. The other usage is image analysis, text synthesis (based upon the long short-term memory architecture).\n",
    "    * The image analysis is very important in fields like, radiology, opthamology, and dermatology in medicine. But the labelled data amount is very limited, which we may need millions of labelled data images. So we are using another method called transfer learning. This concept of **transferring parameters** from a model trained on ImageNet to a different images, this is a significant positive aspect of deep neural networks. The key thing is through this concept of transfer learning, many of the parameters across these arguably very different applications can be transferred almost directly. Only to learn parameters at the top of a network, which are directly applicable to the confocal images. Consequently the number of parameters that we have to learn is significantly reduced, because instead of having to learn all of the parameters, we only learn for the medical images, the new parameters at the top of the network, and then we transfer all of the parameters from the ImageNet.\n",
    "    ![W1.4.3.Applications%20in%20the%20Real%20World%20-%20Transfer%20Learning%202.png](./Week%201/W1.4.3.Applications%20in%20the%20Real%20World%20-%20Transfer%20Learning%202.png)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n",
      "tensor([[0.3153, 0.1667, 0.7799],\n",
      "        [0.1872, 0.5624, 0.3990]])\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install torch\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "x = torch.rand(2,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Plot Logistic Regression**\n",
    "    * My understanding here: traditional regression model $Y_{n*1} = X_{n*m} * \\beta_{m*1} $\n",
    "    * n is the number of observations, m is the number of features or parameters\n",
    "    * we take Y and transpose it as a probablity: $ln\\dfrac {y}{1-y}$\n",
    "    * in another way, $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "x = np.linspace(-6,6,500)\n",
    "\n",
    "y = [1 / (1 + math.exp(-i)) for i in x]\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
